<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Amaia - AI Voice Chat</title>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üé§</text></svg>">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #000; color: #fff; }
        .animate-pulse { animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite; }
        @keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
    </style>
</head>
<body>
    <div id="root"></div>

    <script>
let websocketRef = { current: null };
let audioContextRef = { current: null };
let mediaStreamRef = { current: null };
let isRecordingRef = { current: false };
let isPlayingRef = { current: false };
let audioQueueRef = { current: [] };

const addToTranscript = (speaker, text) => {
    const transcriptDiv = document.getElementById('transcript');
    const entry = document.createElement('div');
    entry.textContent = `${speaker}: ${text}`;
    transcriptDiv.appendChild(entry);
    transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
};

const connectWebSocket = async () => {
    try {
        const response = await fetch('http://localhost:3000/session');
        const data = await response.json();
        websocketRef.current = new WebSocket(data.ws_url);

        websocketRef.current.onopen = () => {
            console.log('‚úÖ Connected to Amaia session');
        };

        websocketRef.current.onmessage = async (event) => {
            const data = JSON.parse(event.data);

            // üé§ USER SPEECH (your side)
            if (data.type === 'user_transcript' && data.user_transcript_event?.transcript) {
                addToTranscript('You', data.user_transcript_event.transcript);
            }

            // ü§ñ AMAIA SPEECH (text output)
            if (data.type === 'agent_response' && data.agent_response_event?.agent_response) {
                addToTranscript('Amaia', data.agent_response_event.agent_response);
            }

            // üîä AMAIA AUDIO (voice output)
            if (data.type === 'audio' && data.audio_event?.audio_chunk) {
                const base64Audio = data.audio_event.audio_chunk;
                const audioBytes = Uint8Array.from(atob(base64Audio), c => c.charCodeAt(0));
                const audioBuffer = audioBytes.buffer;
                audioQueueRef.current.push(audioBuffer);
                playNextAudio();
            }
        };

        websocketRef.current.onclose = () => {
            console.log('‚ùå WebSocket closed, retrying in 3 seconds...');
            setTimeout(connectWebSocket, 3000);
        };

    } catch (err) {
        console.error('‚ùå Failed to connect:', err);
        setTimeout(connectWebSocket, 3000);
    }
};

const startRecording = async () => {
    if (isRecordingRef.current) return;
    isRecordingRef.current = true;

    try {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaStreamRef.current = stream;

        const source = audioContextRef.current.createMediaStreamSource(stream);
        const processor = audioContextRef.current.createScriptProcessor(4096, 1, 1);

        source.connect(processor);
        processor.connect(audioContextRef.current.destination);

        processor.onaudioprocess = (e) => {
            if (!isRecordingRef.current) return;
            const input = e.inputBuffer.getChannelData(0);
            const pcm16 = new Int16Array(input.length);
            for (let i = 0; i < input.length; i++) {
                pcm16[i] = Math.max(-1, Math.min(1, input[i])) * 0x7fff;
            }

            if (websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
                websocketRef.current.send(pcm16);
            }
        };

        console.log('üéôÔ∏è Recording started');
    } catch (err) {
        console.error('‚ùå Microphone access error:', err);
    }
};

const stopRecording = () => {
    isRecordingRef.current = false;
    if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop());
        mediaStreamRef.current = null;
    }
    console.log('üõë Recording stopped');
};

const playNextAudio = async () => {
    if (isPlayingRef.current || audioQueueRef.current.length === 0) return;

    isPlayingRef.current = true;
    const audioData = audioQueueRef.current.shift();

    try {
        const audioContext = audioContextRef.current;
        const audioBuffer = await audioContext.decodeAudioData(audioData.slice(0));

        const source = audioContext.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(audioContext.destination);
        source.onended = () => {
            isPlayingRef.current = false;
            playNextAudio();
        };
        source.start(0);
        console.log('üîä Playing Amaia‚Äôs response');
    } catch (err) {
        console.error('‚ùå Audio decoding failed:', err);
        isPlayingRef.current = false;
        playNextAudio();
    }
};

// Initialize
connectWebSocket();

document.getElementById('startBtn').addEventListener('click', startRecording);
document.getElementById('stopBtn').addEventListener('click', stopRecording);
</script>

</body>
</html>
