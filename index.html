<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Amaia - AI Voice Chat</title>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽ¤</text></svg>">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #000; color: #fff; }
        .animate-pulse { animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite; }
        @keyframes pulse { 0%, 100% { opacity: 1; } 50% { opacity: 0.5; } }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useRef, useEffect } = React;

        function AmaiaPlatform() {
            const [currentPage, setCurrentPage] = useState('intro');
            const [isOnCall, setIsOnCall] = useState(false);
            const [isConnected, setIsConnected] = useState(false);
            const [status, setStatus] = useState('Amaia is online');
            const [callTranscript, setCallTranscript] = useState([]);
            const [error, setError] = useState(null);
            const [audioAllowed, setAudioAllowed] = useState(false);
            const [fallbackAudioUrl, setFallbackAudioUrl] = useState(null);
            const websocketRef = useRef(null);
            const mediaRecorderRef = useRef(null);
            const audioContext = useRef(new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 }));
            const audioQueue = useRef([]);
            const isPlaying = useRef(false);
            const recognitionRef = useRef(null);
            const pingIntervalRef = useRef(null);
            const reconnectAttempts = useRef(0);
            const maxReconnectAttempts = 5;

            // Unlock audio context
            useEffect(() => {
                const unlockAudio = () => {
                    audioContext.current.resume().then(() => {
                        console.log('Audio context unlocked');
                        setAudioAllowed(true);
                    }).catch(e => {
                        console.error('Audio unlock error:', e);
                        setError('Audio permission blocked. Click to enable.');
                    });
                };
                document.addEventListener('click', unlockAudio);
                document.addEventListener('touchstart', unlockAudio);
                return () => {
                    document.removeEventListener('click', unlockAudio);
                    document.removeEventListener('touchstart', unlockAudio);
                };
            }, []);

            // Local speech recognition for UI transcript
            useEffect(() => {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                if (SpeechRecognition) {
                    recognitionRef.current = new SpeechRecognition();
                    recognitionRef.current.continuous = true;
                    recognitionRef.current.interimResults = true;
                    recognitionRef.current.lang = 'en-US';

                    recognitionRef.current.onresult = (event) => {
                        let transcript = '';
                        for (let i = event.resultIndex; i < event.results.length; i++) {
                            transcript += event.results[i][0].transcript + ' ';
                        }
                        console.log('Interim transcript:', transcript);
                        if (event.results[event.results.length - 1].isFinal) {
                            const finalTranscript = event.results[event.results.length - 1][0].transcript;
                            addToTranscript('You', finalTranscript);
                        }
                    };
                    recognitionRef.current.onerror = (event) => {
                        console.error('Speech error:', event.error);
                        setError('Microphone error: ' + event.error);
                    };
                    recognitionRef.current.onend = () => {
                        console.log('Speech recognition ended');
                        if (isOnCall && isConnected) recognitionRef.current.start();
                    };
                } else {
                    setError('Speech recognition not supported.');
                }
            }, [isOnCall, isConnected]);

            const addToTranscript = (speaker, text) => {
                if (text && text.trim()) {
                    setCallTranscript(prev => [...prev, { speaker, text, time: new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' }) }]);
                }
            };

            const createWavBuffer = (audioData) => {
                const sampleRate = 16000;
                const channels = 1;
                const bitsPerSample = 16;
                const byteRate = sampleRate * channels * (bitsPerSample / 8);
                const dataSize = audioData.byteLength;

                const buffer = new ArrayBuffer(44 + dataSize);
                const view = new DataView(buffer);

                // WAV header
                view.setUint32(0, 0x52494646, false); // "RIFF"
                view.setUint32(4, 36 + dataSize, true); // Chunk size
                view.setUint32(8, 0x57415645, false); // "WAVE"
                view.setUint32(12, 0x666d7420, false); // "fmt "
                view.setUint32(16, 16, true); // Subchunk1 size
                view.setUint16(20, 1, true); // PCM format
                view.setUint16(22, channels, true); // Channels
                view.setUint32(24, sampleRate, true); // Sample rate
                view.setUint32(28, byteRate, true); // Byte rate
                view.setUint16(32, channels * (bitsPerSample / 8), true); // Block align
                view.setUint16(34, bitsPerSample, true); // Bits per sample
                view.setUint32(36, 0x64617461, false); // "data"
                view.setUint32(40, dataSize, true); // Data size

                // Copy audio data
                const audioBytes = new Uint8Array(audioData);
                for (let i = 0; i < audioBytes.length; i++) {
                    view.setUint8(44 + i, audioBytes[i]);
                }

                return buffer;
            };

            const playAudioQueue = async () => {
                if (isPlaying.current || audioQueue.current.length === 0) return;
                isPlaying.current = true;
                const chunk = audioQueue.current.shift();
                console.log('Processing audio chunk, size:', chunk.byteLength);

                // Save chunk for debugging
                const debugBlob = new Blob([chunk], { type: 'audio/ogg' });
                const debugUrl = URL.createObjectURL(debugBlob);
                const debugLink = document.createElement('a');
                debugLink.href = debugUrl;
                debugLink.download = `agent_response_${Date.now()}.ogg`;
                debugLink.click();
                console.log('Saved audio chunk as agent_response_[timestamp].ogg');

                // Try direct Opus playback first
                try {
                    console.log('Attempting direct Opus playback...');
                    const audio = new Audio(debugUrl);
                    audio.play().then(() => {
                        console.log('Direct Opus playback started');
                        audio.onended = () => {
                            console.log('Direct Opus playback ended');
                            isPlaying.current = false;
                            playAudioQueue();
                        };
                    }).catch(err => {
                        console.error('Direct Opus playback error:', err.message);
                        throw err; // Fallback to PCM
                    });
                } catch (err) {
                    // Fallback to PCM decoding
                    try {
                        const wavBuffer = createWavBuffer(chunk);
                        console.log('Decoding WAV audio...');
                        const buffer = await audioContext.current.decodeAudioData(wavBuffer);
                        const source = audioContext.current.createBufferSource();
                        source.buffer = buffer;
                        source.connect(audioContext.current.destination);
                        source.start();
                        console.log('PCM audio playback started');
                        source.onended = () => {
                            console.log('PCM audio chunk ended');
                            isPlaying.current = false;
                            playAudioQueue();
                        };
                    } catch (pcmErr) {
                        console.error('PCM decode error:', pcmErr.message);
                        isPlaying.current = false;
                        setFallbackAudioUrl(debugUrl);
                        console.log('Set fallback audio player with URL');
                        playAudioQueue();
                    }
                }
            };

            const getSignedUrl = async () => {
                try {
                    console.log('Fetching signed URL...');
                    const response = await fetch('/api/signed-url');
                    if (!response.ok) throw new Error('Failed to get signed URL');
                    const data = await response.json();
                    console.log('Signed URL received:', data.signedUrl.slice(0, 50) + '...');
                    return data.signedUrl;
                } catch (error) {
                    console.error('Signed URL error:', error);
                    setError('Failed to connect to Amaia. Check console.');
                    return null;
                }
            };

            const startCall = async () => {
                setIsOnCall(true);
                setCurrentPage('call');
                setStatus('Connecting to Amaia...');
                setCallTranscript([{
                    speaker: 'Amaia',
                    text: "Hey, I'm so glad you called. Talk to me.",
                    time: new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })
                }]);
                reconnectAttempts.current = 0;
                connectWebSocket();
            };

            const connectWebSocket = async () => {
                if (reconnectAttempts.current >= maxReconnectAttempts) {
                    setError('Max reconnect attempts reached. Please try again.');
                    setStatus('Disconnected');
                    return;
                }

                const signedUrl = await getSignedUrl();
                if (!signedUrl) return;

                console.log('Connecting to WebSocket:', signedUrl);
                websocketRef.current = new WebSocket(signedUrl);

                websocketRef.current.onopen = async () => {
                    console.log('WebSocket connected');
                    setIsConnected(true);
                    setStatus('Connected! Speak to Amaia.');
                    reconnectAttempts.current = 0;
                    websocketRef.current.send(JSON.stringify({ type: 'conversation_initiation_client_data' }));

                    // Send pre-recorded "hello" Opus audio
                    try {
                        const response = await fetch('https://cdn.pixabay.com/audio/2023/08/07/audio_6c4e518693.mp3');
                        const audioData = await response.arrayBuffer();
                        const audioBlob = new Blob([audioData], { type: 'audio/ogg' });
                        websocketRef.current.send(audioBlob);
                        console.log('Sent initial hello audio, size:', audioBlob.size, 'bytes');
                    } catch (err) {
                        console.error('Failed to send initial hello audio:', err);
                    }

                    startMicrophone();
                    // Start ping
                    pingIntervalRef.current = setInterval(() => {
                        if (websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
                            console.log('Sending ping');
                            websocketRef.current.send(JSON.stringify({ type: 'ping' }));
                        }
                    }, 3000);
                };

                websocketRef.current.onmessage = (event) => {
                    console.log('Received message, type:', typeof event.data);
                    if (typeof event.data === 'string') {
                        try {
                            const data = JSON.parse(event.data);
                            console.log('Received text:', JSON.stringify(data, null, 2));
                            if (data.type === 'agent_response' && data.agent_response_event?.agent_response) {
                                addToTranscript('Amaia', data.agent_response_event.agent_response);
                            } else if (data.type === 'agent_chat_response_part' && data.text_response_part?.text) {
                                addToTranscript('Amaia', data.text_response_part.text);
                            }
                        } catch (err) {
                            console.error('JSON parse error:', err.message);
                            console.log('Non-JSON message:', event.data);
                        }
                    } else if (event.data instanceof Blob) {
                        console.log('Received audio blob, size:', event.data.size);
                        event.data.arrayBuffer().then(buffer => {
                            audioQueue.current.push(buffer);
                            playAudioQueue();
                        }).catch(err => {
                            console.error('Audio buffer error:', err.message);
                            setError('Failed to process audio data');
                        });
                    } else {
                        console.log('Received unknown data type:', typeof event.data);
                        const debugBlob = new Blob([event.data], { type: 'application/octet-stream' });
                        const debugUrl = URL.createObjectURL(debugBlob);
                        const debugLink = document.createElement('a');
                        debugLink.href = debugUrl;
                        debugLink.download = `unknown_response_${Date.now()}.bin`;
                        debugLink.click();
                        console.log('Saved unknown data as unknown_response_[timestamp].bin');
                    }
                };

                websocketRef.current.onerror = (err) => {
                    console.error('WebSocket error:', err);
                    setError('Connection error. Attempting to reconnect...');
                };

                websocketRef.current.onclose = (event) => {
                    console.log('WebSocket closed, code:', event.code, 'reason:', event.reason || 'No reason provided');
                    setIsConnected(false);
                    setStatus('Disconnected');
                    stopMicrophone();
                    if (pingIntervalRef.current) {
                        clearInterval(pingIntervalRef.current);
                        pingIntervalRef.current = null;
                    }
                    if (isOnCall && reconnectAttempts.current < maxReconnectAttempts) {
                        reconnectAttempts.current += 1;
                        console.log(`Reconnecting WebSocket, attempt ${reconnectAttempts.current}/${maxReconnectAttempts}`);
                        setTimeout(connectWebSocket, 1000 * reconnectAttempts.current);
                    }
                };
            };

            const startMicrophone = async () => {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    mediaRecorderRef.current = new MediaRecorder(stream, { mimeType: 'audio/webm;codecs=opus' });
                    mediaRecorderRef.current.ondataavailable = (event) => {
                        if (event.data.size > 0 && websocketRef.current && websocketRef.current.readyState === WebSocket.OPEN) {
                            if (event.data.size < 100) {
                                console.log('Skipping small audio chunk:', event.data.size, 'bytes');
                                return;
                            }
                            console.log('Sent audio chunk, size:', event.data.size, 'type
